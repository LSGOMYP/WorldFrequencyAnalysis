计算机研究与发展ＩＳＳＮ　１０００－１２３９?ＣＮ　１１－１７７７?ＴＰ 
Ｊｏｕｒｎａｌ　ｏｆ　Ｃｏｍｐｕｔｅｒ　Ｒｅｓｅａｒｃｈ　ａｎｄ　Ｄｅｖｅｌｏｐｍｅｎｔ　５０（９）：１７９９－１８０４，２０１３ 
　收稿日期：２０１３－０８－１６；修回日期：２０１３－０８－１９ 
深度学习的昨天、今天和明天
余　凯　贾　磊　陈雨强　徐　伟
（百度　北京　１０００８５） 
（ｙｕｋａｉ＠ｂａｉｄｕ．ｃｏｍ） 
Ｄｅｅｐ　Ｌｅａｒｎｉｎｇ：Ｙｅｓｔｅｒｄａｙ，Ｔｏｄａｙ，ａｎｄ　Ｔｏｍｏｒｒｏｗ 
Ｙｕ　Ｋａｉ，Ｊｉａ　Ｌｅｉ，Ｃｈｅｎ　Ｙｕｑｉａｎｇ，ａｎｄ　Ｘｕ　Ｗｅｉ 
（Ｂａｉｄｕ，Ｂｅｉｊｉｎｇ１０００８５） 
Ａｂｓｔｒａｃｔ　Ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ　ｉｓ　ａｎ　ｉｍｐｏｒｔａｎｔ　ａｒｅａ　ｏｆ　ａｒｔｉｆｉｃｉａｌ　ｉｎｔｅｌｌｉｇｅｎｃｅ．Ｓｉｎｃｅ　１９８０ｓ，ｈｕｇｅ　ｓｕｃｃｅｓｓ 
ｈａｓ　ｂｅｅｎ　ａｃｈｉｅｖｅｄ　ｉｎ　ｔｅｒｍｓ　ｏｆ　ａｌｇｏｒｉｔｈｍｓ，ｔｈｅｏｒｙ，ａｎｄ　ａｐｐｌｉｃａｔｉｏｎｓ．Ｆｒｏｍ　２００６，ａ　ｎｅｗ　ｍａｃｈｉｎｅ 
ｌｅａｒｎｉｎｇ　ｐａｒａｄｉｇｍ，ｎａｍｅｄ　ｄｅｅｐ　ｌｅａｒｎｉｎｇ，ｈａｓ　ｂｅｅｎ　ｐｏｐｕｌａｒ　ｉｎ　ｔｈｅ　ｒｅｓｅａｒｃｈ　ｃｏｍｍｕｎｉｔｙ，ａｎｄ　ｈａｓ 
ｂｅｃｏｍｅ　ａ　ｈｕｇｅ　ｗａｖｅ　ｏｆ　ｔｅｃｈｎｏｌｏｇｙ　ｔｒｅｎｄ　ｆｏｒ　ｂｉｇ　ｄａｔａ　ａｎｄ　ａｒｔｉｆｉｃｉａｌ　ｉｎｔｅｌｌｉｇｅｎｃｅ．Ｄｅｅｐ　ｌｅａｒｎｉｎｇ 
ｓｉｍｕｌａｔｅｓ　ｔｈｅ　ｈｉｅｒａｒｃｈｉｃａｌ　ｓｔｒｕｃｔｕｒｅ　ｏｆ　ｈｕｍａｎ　ｂｒａｉｎ，ｐｒｏｃｅｓｓｉｎｇ　ｄａｔａ　ｆｒｏｍ　ｌｏｗｅｒ　ｌｅｖｅｌ　ｔｏ　ｈｉｇｈｅｒ　ｌｅｖｅｌ， 
ａｎｄ　ｇｒａｄｕａｌｌｙ　ｃｏｍｐｏｓｉｎｇ　ｍｏｒｅ　ａｎｄ　ｍｏｒｅ　ｓｅｍａｎｔｉｃ　ｃｏｎｃｅｐｔｓ．Ｉｎ　ｒｅｃｅｎｔ　ｙｅａｒｓ，Ｇｏｏｇｌｅ，Ｍｉｃｒｏｓｏｆｔ， 
ＩＢＭ，ａｎｄ　Ｂａｉｄｕ　ｈａｖｅ　ｉｎｖｅｓｔｅｄ　ａ　ｌｏｔ　ｏｆ　ｒｅｓｏｕｒｃｅｓ　ｉｎｔｏ　ｔｈｅ　Ｒ＆Ｄ　ｏｆ　ｄｅｅｐ　ｌｅａｒｎｉｎｇ，ｍａｋｉｎｇ　ｓｉｇｎｉｆｉｃａｎｔ 
ｐｒｏｇｒｅｓｓｅｓ　ｏｎ　ｓｐｅｅｃｈ　ｒｅｃｏｇｎｉｔｉｏｎ，ｉｍａｇｅ　ｕｎｄｅｒｓｔａｎｄｉｎｇ，ｎａｔｕｒａｌ　ｌａｎｇｕａｇｅ　ｐｒｏｃｅｓｓｉｎｇ，ａｎｄ　ｏｎｌｉｎｅ 
ａｄｖｅｒｔｉｓｉｎｇ．Ｉｎ　ｔｅｒｍｓ　ｏｆ　ｔｈｅ　ｃｏｎｔｒｉｂｕｔｉｏｎ　ｔｏ　ｒｅａｌ－ｗｏｒｌｄ　ａｐｐｌｉｃａｔｉｏｎｓ，ｄｅｅｐ　ｌｅａｒｎｉｎｇ　ｉｓ　ｐｅｒｈａｐｓ　ｔｈｅ　ｍｏｓｔ 
ｓｕｃｃｅｓｓｆｕｌ　ｐｒｏｇｒｅｓｓ　ｍａｄｅ　ｂｙ　ｔｈｅ　ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ　ｃｏｍｍｕｎｉｔｙ　ｉｎ　ｔｈｅ　ｌａｓｔ　１０ｙｅａｒｓ．Ｉｎ　ｔｈｉｓ　ａｒｔｉｃｌｅ，ｗｅ 
ｗｉｌｌ　ｇｉｖｅ　ａ　ｈｉｇｈ－ｌｅｖｅｌ　ｏｖｅｒｖｉｅｗ　ａｂｏｕｔ　ｔｈｅ　ｐａｓｔ　ａｎｄ　ｃｕｒｒｅｎｔ　ｓｔａｇｅ　ｏｆ　ｄｅｅｐ　ｌｅａｒｎｉｎｇ，ｄｉｓｃｕｓｓ　ｔｈｅ　ｍａｉｎ 
ｃｈａｌｌｅｎｇｅｓ，ａｎｄ　ｓｈａｒｅ　ｏｕｒ　ｖｉｅｗｓ　ｏｎ　ｔｈｅ　ｆｕｔｕｒｅ　ｄｅｖｅｌｏｐｍｅｎｔ　ｏｆ　ｄｅｅｐ　ｌｅａｒｎｉｎｇ． 
Ｋｅｙ　ｗｏｒｄｓ　ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ；ｄｅｅｐ　ｌｅａｒｎｉｎｇ；ｓｐｅｅｃｈ　ｒｅｃｏｇｎｉｔｉｏｎ；ｉｍａｇｅ　ｒｅｃｏｇｎｉｔｉｏｎ；ｎａｔｕｒａｌ　ｌａｎｇｕａｇｅ 
ｐｒｏｃｅｓｓｉｎｇ；ｏｎｌｉｎｅ　ａｄｖｅｒｔｉｓｉｎｇ 
摘　要　机器学习是人工智能领域的一个重要学科．自从２０世纪８０年代以来，机器学习在算法、理论和
应用等方面都获得巨大成功．２００６年以来，机器学习领域中一个叫“深度学习”的课题开始受到学术界
广泛关注，到今天已经成为互联网大数据和人工智能的一个热潮．深度学习通过建立类似于人脑的分层
模型结构，对输入数据逐级提取从底层到高层的特征，从而能很好地建立从底层信号到高层语义的映射
关系．近年来，谷歌、微软、ＩＢＭ、百度等拥有大数据的高科技公司相继投入大量资源进行深度学习技术
研发，在语音、图像、自然语言、在线广告等领域取得显著进展．从对实际应用的贡献来说，深度学习可能
是机器学习领域最近这十年来最成功的研究方向．将对深度学习发展的过去和现在做一个全景式的介
绍，并讨论深度学习所面临的挑战，以及将来的可能方向． 
关键词　机器学习；深度学习；语音识别；图像识别；自然语言处理；在线广告
中图法分类号　ＴＰ１８ 
　　２０１２年６月，《纽约时报》披露了谷歌的Ｇｏｏｇｌｅ 
Ｂｒａｉｎ项目［１］，吸引了公众的广泛关注．这个项目是
由著名的斯坦福大学的机器学习教授Ｎｇ和在大规
模计算机系统方面的世界顶尖专家Ｄｅａｎ共同主导，

用１６　０００个ＣＰＵ　Ｃｏｒｅ的并行计算平台训练一种称
为“深度神经网络”（ｄｅｅｐ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ，ＤＮＮ） 
的机器学习模型，在语音识别和图像识别等领域获
得了巨大的成功．２０１２年１１月，微软在中国天津的
一次活动上公开演示了一个全自动的同声传译系
统，讲演者用英文演讲，后台的计算机一气呵成自动
完成语音识别、英中机器翻译和中文语音合成，效果
非常流畅．据报道，后面支撑的关键技术也是ＤＮＮ， 
或者深度学习（ｄｅｅｐ　ｌｅａｒｎｉｎｇ，ＤＬ）［２］．２０１３年的１ 
月，在中国最大的互联网搜索引擎公司百度的年会
上，创始人兼ＣＥＯ李彦宏高调宣布要成立百度研究
院，其中第一个重点方向的就是深度学习，并为此而
成立Ｉｎｓｔｉｔｕｔｅ　ｏｆ　Ｄｅｅｐ　Ｌｅａｒｎｉｎｇ（ＩＤＬ）．这是百度成
立１０多年以来第一次成立研究院［３］．２０１３年４月， 
《麻省理工学院技术评论》（ＭＩＴ　Ｔｅｃｈｎｏｌｏｇｙ　Ｒｅｖｉｅｗ） 
杂志将深度学习列为２０１３ 年十大突破性技术
（ｂｒｅａｋｔｈｒｏｕｇｈ　ｔｅｃｈｎｏｌｏｇｙ）之首［４］． 
为什么深度学习受到学术届和工业界如此广泛
的重视？深度学习技术研发面临什么样的科学和工
程问题？深度学习带来的科技进步将怎样改变人们
的生活？本文将简要回顾机器学习在过去２０多年
的发展，介绍深度学习的昨天、今天和明天． 
１　机器学习的两次浪潮：从浅层学习到深度
学习
　　在解释深度学习之前，我们需要了解什么是机
器学习．机器学习是人工智能的一个分支，而在很多
时候，几乎成为人工智能的代名词．简单来说，机器
学习就是通过算法，使得机器能从大量历史数据中
学习规律，从而对新的样本做智能识别或对未来做
预测．从２０世纪８０年代末期以来，机器学习的发展
大致经历了两次浪潮：浅层学习（ｓｈａｌｌｏｗ　ｌｅａｒｎｉｎｇ） 
和深度学习（ｄｅｅｐ　ｌｅａｒｎｉｎｇ）．需要指出是，机器学习
历史阶段的划分是一个仁者见仁，智者见智的事情， 
从不同的维度来看会得到不同的结论．这里我们是
从机器学习模型的层次结构来看的． 
１．１　第一次浪潮：浅层学习
２０世纪８０年代末期，用于人工神经网络的反
向传播算法（也叫Ｂａｃｋ　Ｐｒｏｐａｇａｔｉｏｎ算法或者ＢＰ 
算法）的发明，给机器学习带来了希望，掀起了基于
统计模型的机器学习热潮［５］．这个热潮一直持续到
今天．人们发现，利用ＢＰ算法可以让一个人工神经
网络模型从大量训练样本中学习出统计规律，从而
对未知事件做预测．这种基于统计的机器学习方法
比起过去基于人工规则的系统，在很多方面显示出
优越性．这个时候的人工神经网络，虽然也被称作多
层感知机（ｍｕｌｔｉ－ｌａｙｅｒ　ｐｅｒｃｅｐｔｒｏｎ），由于多层网络
训练的困难，实际使用的多数是只含有一层隐层节
点的浅层模型． 
２０世纪９０年代，各种各样的浅层机器学习
模型相继被提出，比如支撑向量机（ｓｕｐｐｏｒｔ　ｖｅｃｔｏｒ 
ｍａｃｈｉｎｅｓ，ＳＶＭ），Ｂｏｏｓｔｉｎｇ，最大熵方法（比如
ｌｏｇｉｓｔｉｃ　ｒｅｇｒｅｓｓｉｏｎ，ＬＲ）等．这些模型的结构基本上
可以看成带有一层隐层节点（如ＳＶＭ，Ｂｏｏｓｔｉｎｇ）， 
或没有隐层节点（如ＬＲ）．这些模型在无论是理论分
析还是应用都获得了巨大的成功．相比较之下，由于
理论分析的难度，而且训练方法需要很多经验和技
巧，这个时期多层人工神经网络反而相对较为沉寂． 
２０００年以来互联网的高速发展，对大数据的智
能化分析和预测提出了巨大需求，浅层学习模型在
互联网应用上获得了巨大的成功．最成功的应用包
括搜索广告系统（比如谷歌的Ａｄｗｏｒｄｓ、百度的凤
巢系统）的广告点击率ＣＴＲ 预估、网页搜索排序
（比如雅虎和微软的搜索引擎）、垃圾邮件过滤系统、
基于内容的推荐系统，等等． 
１．２　第二次浪潮：深度学习
２００６年，加拿大多伦多大学教授，机器学习领
域的泰斗Ｈｉｎｔｏｎ和他的学生Ｓａｌａｋｈｕｔｄｉｎｏｖ在顶
尖学术刊物《科学》上发表了一篇文章［６］，开启了深
度学习在学术界和工业界的浪潮．这篇文章有两个
主要的讯息：１）很多隐层的人工神经网络具有优异
的特征学习能力，学习得到的特征对数据有更本质
的刻划，从而有利于可视化或分类；２）深度神经网络
在训练上的难度，可以通过" 逐层初始化”（ｌａｙｅｒｗｉｓｅ
　ｐｒｅ－ｔｒａｉｎｉｎｇ）来有效克服，在这篇文章中，逐层
初始化是通过无监督学习实现的． 
自２００６年以来，深度学习在学术界持续升温． 
斯坦福大学、纽约大学、加拿大蒙特利尔大学等成为
研究深度学习的重镇．２０１０年，美国国防部ＤＡＲＰＡ 
计划首次资助深度学习项目，参与方有斯坦福大学、
纽约大学和ＮＥＣ美国研究院．支持深度学习的一
个重要依据，就是脑神经系统的确具有丰富的层次
结构．一个最著名的例子就是Ｈｕｂｅｌ－Ｗｉｅｓｅｌ模型， 
由于揭示了视觉神经的机理而曾获得诺贝尔医学与
生理学奖．除了仿生学的角度，目前深度学习的理论
研究还基本处于起步阶段，但在应用领域已经显现
巨大能量．２０１１年以来，微软研究院和谷歌的语音
１８００ 计算机研究与发展　２０１３，５０（９）

识别研究人员先后采用ＤＮＮ技术降低语音识别错
误率２０％～３０％，是语音识别领域１０多年来最大
的突破性进展．２０１２年ＤＮＮ技术在图像识别领域
取得惊人的效果，在ＩｍａｇｅＮｅｔ评测上将错误率从
２６％降低到１５％．在这一年，ＤＮＮ还被应用于制药
公司的Ｄｒｕｇｅ　Ａｃｔｉｖｉｔｙ预测问题，并获得世界最好
成绩，这一重要成果被《纽约时报》报道． 
正如文章开头所描述的，今天谷歌、微软、百度
等知名的拥有大数据的高科技公司争相投入资源， 
占领深度学习的技术制高点，正是因为他们都看到
了大数据时代，更加复杂且更加强大的深度模型的
能深刻揭示海量数据里所承载的负责而丰富的信
息，并对未来或未知事件做更精准的预测． 
２　大数据与深度学习
在工业界一直有一个很流行的观点：在大数据
条件下，简单的机器学习模型会比复杂模型更加有
效．比如说，在很多的大数据应用中，最简单的线性
模型得到大量使用．而最近深度学习的惊人进展促
使我们也许到了要重新思考这个观点的时候．简而
言之，在大数据情况下，也许只有比较复杂的模型， 
或者说表达能力强的模型，才能够充分发掘海量数
据中蕴藏的丰富信息．现在我们到了需要重新思考
“大数据＋简单模型”的时候．运用更强大的深度模
型，也许我们能从大数据中发掘出更多的有价值的
信息和知识． 
为了理解为什么大数据需要深度模型，先举一
个例子．语音识别已经是一个大数据的机器学习问
题，在其声学建模部分，通常面临的是十亿到千亿级
别的训练样本．在谷歌的一个语音识别实验中，发现
训练后的ＤＮＮ对训练样本和测试样本的预测误差
基本相当．这是非常违反常识的，因为通常模型在训
练样本上的预测误差会显著小于测试样本．只有一
个解释，就是由于大数据里含有丰富的信息维度，即
便是ＤＮＮ这样的高容量复杂模型也是处于欠拟合
的状态，更不必说传统的ＧＭＭ 声学模型了．所以
在这个例子里我们看出，大数据需要深度学习． 
浅层模型有一个重要特点，就是假设靠人工经
验来抽取样本的特征，而强调模型主要是负责分类
或预测．在模型的运用不出差错的前提下（比如，假
设互联网公司聘请的是机器学习的专家），特征的好
坏就成为整个系统性能的瓶颈．因此，通常一个开发
团队中更多的人力是投入到发掘更好的特征上去
的．发现一个好的特征，要求开发人员对待解决的问
题要有很深入的理解．而达到这个程度，往往需要反
复的摸索，甚至是数年磨一剑．因此，人工设计样本
特征，不是一个可扩展的途径． 
深度学习的实质，是通过构建具有很多隐层的
机器学习模型和海量的训练数据，来学习更有用的
特征，从而最终提升分类或预测的准确性．所以，“深
度模型”是手段，“特征学习”是目的．区别于传统的
浅层学习，深度学习的不同在于：１）强调了模型结构
的深度，通常有５层、６层、甚至１０多层的隐层节
点；２）明确突出了特征学习的重要性，也就是说，通
过逐层特征变换，将样本在原空间的特征表示变换
到一个新特征空间，从而分类或预测更加容易． 
与人工规则构造特征的方法相比，利用大数据
来学习特征，更能够刻划数据的丰富内在信息．所
以，在未来的几年里，我们将看到越来越多的例子， 
深度模型应用于大数据，而不是浅层的线性模型． 
３　深度学习的应用
３．１　语音识别
语音识别系统长期以来，描述每个建模单元的
统计概率模型时候，大都是采用的混合高斯模型
（ＧＭＭ）．这种模型由于估计简单，适合海量数据训
练，同时有成熟的区分度训练技术支持，长期以来， 
一直在语音识别应用中占有垄断性地位．但是这种
混合高斯模型本质上是一种浅层网络建模，不能够
充分描述特征的状态空间分布．另外，ＧＭＭ 建模的
特征维数一般是几十维，不能充分描述特征之间的
相关性．最后ＧＭＭ 建模本质上是一种似然概率建
模，虽然区分度训练能够模拟一些模式类之间的区
分性，但是能力有限． 
微软研究院的语音识别专家Ｌｉ和Ｄｏｎｇ从
２００９年开始和深度学习专家Ｈｉｎｔｏｎ合作．２０１１年
微软基于深度神经网络的语音识别研究取得成果， 
彻底底改变了语音识别原有的技术框架［７］．采用深
度神经网络后，可以充分描述特征之间的相关性，可
以把连续多帧的语音特征并在一起，构成一个高维
特征．最终的深度神经网络可以采用高维特征训练
来模拟的．由于深度神经网络采用模拟人脑的多层
结果，可以逐级地进行信息特征抽取，最终形成适合
模式分类的较理想特征．这种多层结构和人脑处理
语音图像信息的时候，是有很大的相似性的．深度神
经网络的建模技术，在实际线上服务时，能够无缝地
余　凯等：深度学习的昨天、今天和明天 １８０１

和传统的语音识别技术相结合，在不引起任何系统
额外耗费情况下大幅度地提升了语音识别系统的识
别率．其在线的使用方法具体如下：在实际解码过程
中，声学模型仍然是采用传统的ＨＭＭ 模型，语音
模型仍然是采用传统的统计语言模型，解码器仍然
是采用传统的动态ＷＦＳＴ解码器．但是在声学模型
的输出分布计算时，完全用神经网络的输出后验概
率除以一个先验概率来代替传统ＨＭＭ 模型中的
ＧＭＭ 的输出似然概率．百度实践中发现，采用
ＤＮＮ进行声音建模的语音识别系统的相比于传统
的ＧＭＭ 语音识别系统而言，相对误识别率能降低
２５％．最终在２０１２年１１月的时候，上线了第一款基
于ＤＮＮ的语音搜索系统，成为最早采用ＤＮＮ技术
进行商业语音服务的公司之一． 
国际上谷歌也采用了深度神经网络进行声音建
模，和百度一起是最早的突破深度神经网络工业化
应用的企业之一．但是谷歌产品中采用的深度神经
网络有４～５层［８］，而百度采用的深度神经网络多达
９层．这种结构差异的核心其实是百度更好的解决
了深度神经网络在线计算的技术难题，从而百度线
上产品可以采用更复杂的网络模型．这将对于未来
拓展海量语料的ＤＮＮ模型训练有更大的优势． 
３．２　图像识别
图像是深度学习最早尝试的应用领域．早在
１９８９年，ＬｅＣｕｎ（现纽约大学教授）和他的同事们
就发表了卷积神经网络（ｃｏｎｖｏｌｕｔｉｏｎ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ， 
ＣＮＮ）的工作［９］．ＣＮＮ是一种带有卷积结构的深度
神经网络，通常至少有２个非线性可训练的卷积层、
２个非线性的固定卷积层（又叫ｐｏｏｌｉｎｇ　ｌａｙｅｒ）和１ 
个全连接层，一共至少５个隐含层．ＣＮＮ 的结构受
到著名的Ｈｕｂｅｌ－Ｗｉｅｓｅｌ生物视觉模型的启发，尤其
是模拟视觉皮层Ｖ１ 和Ｖ２ 层中ｓｉｍｐｌｅ　ｃｅｌｌ和
ｃｏｍｐｌｅｘ　ｃｅｌｌ的行为．在很长时间里，ＣＮＮ虽然在小
规模的问题上，比如说手写数字，取得当时世界最好
结果，但一直没有取得巨大成功．这主要原因是
ＣＮＮ在大规模图像上效果不好，比如像素很多的自
然图片内容理解，所以没有得到计算机视觉领域的
足够重视．这个情况一直持续到２０１２ 年１０ 月， 
Ｈｉｎｔｏｎ和他的两个学生在著名的ＩｍａｇｅＮｅｔ问题上
用更深的ＣＮＮ 取得世界最好结果，使得图像识别
大踏步前进［１０］．在Ｈｉｎｔｏｎ的模型里，输入就是图像
的像素，没有用到任何的人工特征． 
这个惊人的结果为什么在之前没有发生？原因
当然包括算法的提升，比如ｄｒｏｐｏｕｔ等防止过拟合
技术，但最重要的是ＧＰＵ 带来的计算能力提升和
更多的训练数据．百度在２０１２年底将深度学习技术
成功应用于自然图像ＯＣＲ 识别和人脸识别等问
题，并推出相应的桌面和移动搜索产品，在２０１３年， 
深度学习模型被成功应用于一般图片的识别和理
解．从百度的经验来看，深度学习应用于图像识别不
但大大提升了准确性，而且避免了人工特征抽取的
时间消耗，从而大大提高了在线计算效率．可以很有
把握地说，从现在开始，深度学习将取代人工特征＋ 
机器学习的方法而逐渐成为主流图像识别方法． 
３．３　自然语言处理
除了语音和图像，深度学习的另一个应用领域
问题自然语言处理（ＮＬＰ）．经过几十年的发展，基于
统计的模型已经成为ＮＬＰ的主流，但是作为统计方
法之一的人工神经网络在ＮＬＰ领域几乎没有受到
重视．本文作者之一徐伟曾最早应用神经网络于语
言模型［１１］．加拿大蒙特利尔大学教授Ｂｅｎｇｉｏ等于
２００３年提出用ｅｍｂｅｄｄｉｎｇ的方法将词映射到一个
矢量表示空间，然后用非线性神经网络来表示ＮＧｒａｍ
模型［１２］．世界上最早的深度学习用于ＮＬＰ的
研究工作诞生于ＮＥＣ　Ｌａｂｓ　Ａｍｅｒｉｃａ［１３］，其研究员
Ｃｏｌｌｏｂｅｒｔ和Ｗｅｓｔｏｎ从２００８年开始采用ｅｍｂｅｄｄｉｎｇ 
和多层一维卷积的结构，用于ＰＯＳ　ｔａｇｇｉｎｇ，Ｃｈｕｎｋｉｎｇ，
Ｎａｍｅｄ　Ｅｎｔｉｔｙ　Ｒｅｃｏｇｎｉｔｉｏｎ，Ｓｅｍａｎｔｉｃ　Ｒｏｌｅ 
Ｌａｂｅｌｉｎｇ等４个典型ＮＬＰ问题．值得注意的是，他
们将同一个模型用于不同任务，都能取得与ｓｔａｔｅｏｆ－
ｔｈｅ－ａｒｔ相当的准确率．最近以来，斯坦福大学教
授Ｍａｎｎｉｎｇ等人在深度学习用于ＮＬＰ的工作也值
得关注［１４］． 
总的来说，深度学习在ＮＬＰ上取得的进展没有
在语音图像上那么令人影响深刻．一个很有意思的
悖论是：相比于声音和图像，语言是唯一的非自然信
号，是完全由人类大脑产生和处理的符号系统，但是
模仿人脑结构的人工神经网络确似乎在处理自然语
言上没有显现明显优势？我们相信深度学习在ＮＬＰ 
方面有很大的探索空间．从２００６年图像深度学习成
为学术界热门课题到２０１２ 年１０ 月Ｈｉｎｔｏｎ 在
ＩｍａｇｅＮｅｔ上的重大突破，经历了６年时间．我们需
要有足够耐心． 
３．４　搜索广告ＣＴＲ预估
搜索广告是搜索引擎的主要变现方式，而按点
击付费（ｃｏｓｔ　ｐｅｒ　ｃｌｉｃｋ，ＣＰＣ）又是其中被最广泛应
用的计费模式．在ＣＰＣ模式下，预估的ＣＴＲ（ｐＣＴＲ） 
越准确，点击率就会越高，收益就越大．通常，搜索广
１８０２ 计算机研究与发展　２０１３，５０（９）

告的ｐＣＴＲ 是通过机器学习模型预估得到．提高
ｐＣＴＲ的准确性，是提升搜索公司、广告主、搜索用
户三方利益的最佳途径． 
传统上，谷歌、百度等搜索引擎公司以ｌｏｇｉｓｔｉｃ 
ｒｅｇｒｅｓｓｉｏｎ（ＬＲ）作为预估模型．而从２０１２年开始， 
百度开始意识到模型的结构对广告ＣＴＲ预估的重
要性：使用扁平结构的ＬＲ严重限制了模型学习与
抽象特征的能力．为了突破这样的限制，百度尝试将
ＤＮＮ作用于搜索广告，而这其中最大的挑战在于当
前的计算能力还无法接受１０１１级别的原始广告特征
作为输入．作为解决，在百度的ＤＮＮ 系统里，特征
数从１０１１数量级被降到了１０３，从而能被ＤＮＮ正常
的学习．这套深度学习系统已于２０１３年５月开始服
务于百度搜索广告系统，每天为数亿网民使用． 
ＤＮＮ在搜索广告系统中的应用还远远没到成
熟，其中ＤＮＮ与迁移学习的结合将可能是一个令
人振奋的方向．使用ＤＮＮ，未来的搜索广告将可能
借助网页搜索的结果优化特征的学习与提取；亦可
能通过ＤＮＮ将不同的产品线联系起来，使得不同
的变现产品不管数据多少，都能互相优化．我们认为
未来的ＤＮＮ 一定会在搜索广告中起到更重要的
作用． 
４　深度学习研发面临的重大问题
４．１　理论问题
理论问题主要体现在两个方面，一个是统计学
习方面的，另一个是计算方面的．我们已经知道，深
度模型相比较于浅层模型有更好的对非线性函数的
表示能力．具体来说，对于任意一个非线性函数，根
据神经网络的ｕｎｉｖｅｒｓａｌ　ａｐｐｒｏｘｉｍａｔｉｏｎ　ｔｈｅｏｒｙ，我们
一定能找到一个浅层网络和一个深度网络来足够好
的表示．但是某些类函数，深度网络只需要少得多的
参数就可以表示．但是，可表示性不代表可学习性． 
我们需要了解深度学习的样本复杂度，也就是我们
需要多少训练样本才能学习到足够好的深度模型． 
在另一方面来说，我们需要多少的计算资源才能通
过训练得到更好的模型，理想的计算优化方法是什
么？由于深度模型都是非凸函数，这方面的理论研
究极其困难． 
４．２　建模问题
在推进深度学习的学习理论和计算理论的同时， 
我们是否可以提出新的分层模型，使其不但具有传
统深度模型所具有的强大表示能力，而且具有其他
的好处，比如更容易做理论分析．另外，针对具体应
用问题，我们如何设计一个最适合的深度模型来解
决问题？我们已经看到，无论在图像深度模型，还是
语言深度模型，似乎都存在深度和卷积等共同的信
息处理结构．甚至对于语音声学模型，研究人员也在
探索卷积深度网络．那么一个更有意思的问题是，是
否存在可能建立一个通用的深度模型或深度模型的
建模语言，作为统一的框架来处理语音、图像和语言？ 
另外，对于怎么用深度模型来表示象语义这样
的结构化的信息还需要更多的研究．从人类进化的
角度来看，语言的能力是远远滞后于视觉和听觉的
能力而发展的．而除了人类以外，还有很多动物具有
很好的识别物体和声音的能力．因此从这个角度来
说，对于神经网络这样的结构而言，语言相较于视觉
和听觉是更为困难的一个任务．而成功的解决这个
难题对于实现人工智能是不可缺少的一步． 
４．３　工程问题
需要指出的是，对于互联网公司而言，如何在工
程上利用大规模的并行计算平台来实现海量数据训
练，是各个公司从事深度学习技术研发首先要解决
的问题．传统的大数据平台如Ｈａｄｏｏｐ，由于数据处
理的ｌａｔｅｎｃｙ太高，显然不适合需要频繁迭代的深度
学习．现有成熟的ＤＮＮ 训练技术大都是采用随机
梯度法（ＳＧＤ）方法训练的．这种方法本身不可能在
多个计算机之间并行．即使是采用ＧＰＵ 进行传统
的ＤＮＮ模型进行训练，其训练时间也是非常漫长
的．一般训练几千小时的声学模型所需要几个月的
时间．而随着互联网服务的深入，海量数据训练越来
越重要，ＤＮＮ这种缓慢的训练速度必然不能满足互
联网服务应用的需要．谷歌搭建的ＤｉｓｔＢｅｌｉｅｆ，是一
个采用普通服务器的深度学习并行计算平台，采用
异步算法，由很多的计算单元独立的更新同一个参
数服务器的模型参数，实现了随机梯度下降算法的
并行化，加快了模型训练速度．与谷歌采用普通服务
器不同，百度的多ＧＰＵ 并行计算的计算平台，克服
了传统ＳＧＤ训练的不能并行的技术难题，神经网络
的训练已经可以在海量语料上并行展开．可以预期
未来随着海量数据训练的ＤＮＮ 技术的发展，语音
图像系统的识别率还会持续提升． 
目前最大的深度模型所包含的参数大约在１００ 
亿的数量级，还不及人脑的万分之一．而由于计算成
本的限制，实际运用于产品中的深度模型更是远远
低于这个水平．而深度模型的一个巨大优势在于，在
有海量数据的情况下，很容易通过增大模型来达到
余　凯等：深度学习的昨天、今天和明天 １８０３

更高的准确率．因此，发展适合深度模型的更高速的
硬件也将是提高深度模型的识别率的重要方向． 
５　总　　结
深度学习带来了机器学习的一个新浪潮，受到
从学术届到工业界的广泛重视，也导致了“大数据＋ 
深度模型”时代的来临．在应用方面，深度学习使得
语音图像的智能识别和理解取得惊人进展，从而推
动人工智能和人机交互大踏步前进．同时ｐＣＴＲ这
样的复杂机器学习任务也得到显著提升．如果我们
能在理论、建模和工程方面，突破深度学习技术面
临一系列难题，我们将大大加速推进人工智能向前
发展． 
参考文献
［１］ Ｍａｒｋｏｆｆ　Ｊ．Ｈｏｗ　ｍａｎｙ　ｃｏｍｐｕｔｅｒｓ　ｔｏ　ｉｄｅｎｔｉｆｙ　ａ　ｃａｔ？［Ｎ］Ｔｈｅ 
Ｎｅｗ　Ｙｏｒｋ　Ｔｉｍｅｓ，２０１２－０６－２５ 
［２］ Ｍａｒｋｏｆｆ　Ｊ．Ｓｃｉｅｎｔｉｓｔｓ　ｓｅｅ　ｐｒｏｍｉｓｅ　ｉｎ　ｄｅｅｐ－ｌｅａｒｎｉｎｇ　ｐｒｏｇｒａｍｓ 
［Ｎ］．Ｔｈｅ　Ｎｅｗ　Ｙｏｒｋ　Ｔｉｍｅｓ，２０１２－１１－２３ 
［３］ Ｌｉ　Ｙａｎｈｏｎｇ．Ｔｏ　ｂｅｌｉｅｖｅ　ｉｎ　ｔｈｅ　ｐｏｗｅｒ　ｏｆ　ｔｅｃｈｎｏｌｏｇｙ［Ｒ］． 
Ｂｅｉｊｉｎｇ：Ｂａｉｄｕ，２０１３（ｉｎ　Ｃｈｉｎｅｓｅ） 
（李彦宏．２０１２百度年会主题报告：相信技术的力量［Ｒ］．北
京：百度，２０１３） 
［４］ １０Ｂｒｅａｋｔｈｒｏｕｇｈ　Ｔｅｃｈｎｏｌｏｇｉｅｓ　２０１３［Ｎ］．ＭＩＴ　Ｔｅｃｈｎｏｌｏｇｙ 
Ｒｅｖｉｅｗ，２０１３－０４－２３ 
［５］ Ｒｕｍｅｌｈａｒｔ　Ｄ， Ｈｉｎｔｏｎ　Ｇ， Ｗｉｌｌｉａｍｓ　Ｒ． Ｌｅａｒｎｉｎｇ 
ｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ　ｂｙ　ｂａｃｋ－ｐｒｏｐａｇａｔｉｎｇ　ｅｒｒｏｒｓ ［Ｊ］．Ｎａｔｕｒｅ， 
１９８６，３２３（６０８８）：５３３－５３６ 
［６］ Ｈｉｎｔｏｎ　Ｇ，Ｓａｌａｋｈｕｔｄｉｎｏｖ　Ｒ．Ｒｅｄｕｃｉｎｇ　ｔｈｅ　ｄｉｍｅｎｓｉｏｎａｌｉｔｙ　ｏｆ 
ｄａｔａ　ｗｉｔｈ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ［Ｊ］．Ｓｃｉｅｎｃｅ，２００６，３１３（５０４）． 
Ｄｏｉ：１０．１１２６?ｓｃｉｅｎｃｅ．１１２７６４７ 
［７］ Ｄａｈｌ　Ｇ，Ｙｕ　Ｄｏｎｇ，Ｄｅｎｇ　Ｌｉ，ｅｔ　ａｌ．Ｃｏｎｔｅｘｔ－ｄｅｐｅｎｄｅｎｔ　ｐｒｅｔｒａｉｎｅｄ
　ｄｅｅｐ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ　ｆｏｒ　ｌａｒｇｅ　ｖｏｃａｂｕｌａｒｙ　ｓｐｅｅｃｈ 
ｒｅｃｏｇｎｉｔｉｏｎ ［Ｊ］．ＩＥＥＥ　Ｔｒａｎｓ　ｏｎ　Ａｕｄｉｏ， Ｓｐｅｅｃｈ，ａｎｄ 
Ｌａｎｇｕａｇｅ　Ｐｒｏｃｅｓｓｉｎｇ．２０１２，２０（１）：３０－４２ 
［８］ Ｊａｉｔｌｙ　Ｎ，Ｎｇｕｙｅｎ　Ｐ，Ｎｇｕｙｅｎ　Ａ，ｅｔ　ａｌ．Ａｐｐｌｉｃａｔｉｏｎ　ｏｆ 
ｐｒｅｔｒａｉｎｅｄ　ｄｅｅｐ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ　ｔｏ　ｌａｒｇｅ　ｖｏｃａｂｕｌａｒｙ　ｓｐｅｅｃｈ 
ｒｅｃｏｇｎｉｔｉｏｎ［Ｃ］??Ｐｒｏｃ　ｏｆ　Ｉｎｔｅｒｓｐｅｅｃｈ，Ｇｒｅｎｏｂｌｅ，Ｆｒａｎｃｅ： 
Ｉｎｔｅｒｎａｔｉｏｎａｌ　Ｓｐｅｅｃｈ　Ｃｏｍｍｕｎｉｃａｔｉｏｎ　Ａｓｓｏｃｉａｔｉｏｎ，２０１２ 
［９］ ＬｅＣｕｎ　Ｙ，Ｂｏｓｅｒ　Ｂ，Ｄｅｎｋｅｒ　Ｊ　Ｓ，ｅｔ　ａｌ．Ｂａｃｋｐｒｏｐａｇａｔｉｏｎ 
ａｐｐｌｉｅｄ　ｔｏ　ｈａｎｄｗｒｉｔｔｅｎ　ｚｉｐ　ｃｏｄｅ　ｒｅｃｏｇｎｉｔｉｏｎ ［Ｊ］．Ｎｅｕｒａｌ 
Ｃｏｍｐｕｔａｔｉｏｎ，１９８９，１：５４１－５５１ 
［１０］ Ｌａｒｇｅ　Ｓｃａｌｅ　Ｖｉｓｕａｌ　Ｒｅｃｏｇｎｉｔｉｏｎ　Ｃｈａｌｌｅｎｇｅ　２０１２ 
（ＩＬＳＶＲＣ２０１２）［ＯＬ］．［２０１３－０８－０１］．ｈｔｔｐ：??ｗｗｗ．ｉｍａｇｅｎｅｔ．
ｏｒｇ?ｃｈａｌｌｅｎｇｅｓ?ＬＳＶＲＣ?２０１２? 
［１１］ Ｘｕ　Ｗ，Ｒｕｄｎｉｃｋｙ　Ａ．Ｃａｎ　ａｒｔｉｆｉｃｉａｌ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋ　ｌｅａｒｎ 
ｌａｎｇｕａｇｅ　ｍｏｄｅｌｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｓｔａｔｉｓｔｉｃａｌ 
Ｌａｎｇｕａｇｅ　Ｐｒｏｃｅｓｓｉｎｇ．２０００：１－１３ 
［１２］ Ｂｅｎｇｉｏ　Ｙ，Ｄｕｃｈａｒｍｅ　Ｒ， Ｖｉｎｃｅｎｔ　Ｐ，ｅｔ　ａｌ． Ａ　ｎｅｕｒａｌ 
ｐｒｏｂａｂｉｌｉｓｔｉｃ　ｌａｎｇｕａｇｅ　ｍｏｄｅｌ ［Ｊ］． Ｊｏｕｒｎａｌ　ｏｆ　Ｍａｃｈｉｎｅ 
Ｌｅａｒｎｉｎｇ　Ｒｅｓｅａｒｃｈ，２００３，３：１１３７－１１５５ 
［１３］ Ｃｏｌｌｏｂｅｒｔ　Ｒ，Ｗｅｓｔｏｎ　Ｊ，Ｂｏｔｔｏｕ　Ｌ，ｅｔ　ａｌ．Ｎａｔｕｒａｌ　ｌａｎｇｕａｇｅ 
ｐｒｏｃｅｓｓｉｎｇ（Ａｌｍｏｓｔ）ｆｒｏｍ　ｓｃｒａｔｃｈ［Ｊ］．Ｊｏｕｒｎａｌ　ｏｆ　Ｍａｃｈｉｎｅ 
Ｌｅａｒｎｉｎｇ　Ｒｅｓｅａｒｃｈ，２０１１，１２：２４９３－２５３７ 
［１４］ Ｓｏｃｈｅｒ　Ｒ，Ｌｉｎ　Ｃ，Ｎｇ　Ａ．Ｐａｒｓｉｎｇ　ｎａｔｕｒａｌ　ｓｃｅｎｅｓ　ａｎｄ　ｎａｔｕｒａｌ 
ｌａｎｇｕａｇｅ　ｗｉｔｈ　ｒｅｃｕｒｓｉｖｅ　ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ 
２８ｔｈ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｍａｃｈｉｎｅ　Ｌｅａｒｎｉｎｇ．Ｇａｒｍａｎｙ：Ｉｎｔｅｒｎａｔｉｏｎａｌ 
Ｍａｃｈｉｎｅ　Ｌｅａｒｎｉｎｇ　Ｓｏｃｉｅｔｙ，２０１１ 
Ｙｕ　Ｋａｉ． Ｒｅｃｅｉｖｅｄ　ｈｉｓ　ＰｈＤ　ｆｒｏｍ　ｔｈｅ 
Ｕｎｉｖｅｒｓｉｔｙ　ｏｆ　Ｍｕｎｉｃｈ　ｉｎ　２００４．Ｐｒｏｆｅｓｓｏｒ． 
Ｈｉｓ　ｍａｉｎ　ｒｅｓｅａｒｃｈ　ｉｎｔｅｒｅｓｔｓ　ｉｎｃｌｕｄｅ 
ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ，ｉｍａｇｅ　ｐｒｏｃｅｓｓｉｎｇ，ｅｔｃ． 
Ｊｉａ　Ｌｅｉ．Ｒｅｃｅｉｖｅｄ　ｈｉｓ　ＰｈＤ　ｄｅｇｒｅｅ　ｉｎ　ｓｐｅｅｃｈ 
ｒｅｃｏｇｎｉｔｉｏｎ　ｆｒｏｍ　ｔｈｅ　Ｉｎｓｔｉｔｕｔｅ　ｏｆ 
Ａｕｔｏｍａｔｉｏｎ， Ｃｈｉｎｅｓｅ　Ａｃａｄｅｍｙ　ｏｆ 
Ｓｃｉｅｎｃｅｓ，ｉｎ　２００３． Ｈｉｓ　ｍａｉｎ　ｒｅｓｅａｒｃｈ 
ｉｎｔｅｒｅｓｔｓ　ｉｎｃｌｕｄｅ　ｓｐｅｅｃｈ　ｒｅｃｏｇｎｉｔｉｏｎ， 
ｍａｃｈｉｎｅ　ｌｅａｒｎｎｉｎｇ，ｄｉｇｉｔａｌ　ｓｉｇｎａｌ　ｐｒｏｃｅｓｓｉｎｇ　ａｎｄ　ｎａｔｕａｌ 
ｌａｎｇｕａｇｅ　ｐｒｏｃｅｓｓｉｎｇ． 
Ｃｈｅｎ　Ｙｕｑｉａｎｇ．Ｒｅｃｅｉｖｅｄ　ｈｉｓ　Ｍａｓｔｅｒ　ｄｅｇｒｅｅ 
ｆｒｏｍ　Ｓｈａｎｇｈａｉ　Ｊｉａｏｔｏｎｇ　Ｕｎｉｖｅｒｓｉｔｙ． 
Ｅｎｇｉｎｅｅｒ． Ｈｉｓ　ｍａｉｎ　ｒｅｓｅａｒｃｈ　ｉｎｔｅｒｅｓｔｓ 
ｉｎｃｌｕｄｅ　ｍａｃｈｉｎｇ　ｌｅａｒｎｉｎｇ． 
Ｘｕ　Ｗｅｉ．Ｒｅｃｅｉｖｅｄ　ｈｉｓ　Ｍａｓｔｅｒ　ｄｅｇｒｅｅ　ｆｒｏｍ 
Ｃａｒｎｅｇｉｅ　Ｍｅｌｌｏｎ　Ｕｎｉｖｅｒｓｉｔｙ　ｉｎ　２００１．Ｈｉｓ 
ｍａｉｎ　ｒｅｓｅａｒｃｈ　ｉｎｔｅｒｅｓｔｓ　ｉｎｃｌｕｄｅ　ｃｏｍｐｕｔｅｒ 
ｖｉｓｉｏｎ，ｄａｔａ　ｍｉｎｉｎｇ， ｍａｃｈｉｎｇ　ｌｅａｒｎｉｎｇ 
ｅｓｐｅｃｉａｌｌｙ　ｄｅｅｐ　ｌｅａｒｎｉｎｇ． 
１８０４ 计算机研究与发展　２０１３，５０（９）

