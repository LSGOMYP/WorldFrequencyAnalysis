基于词频统计的中文分词的研究


费洪晓康松林朱小娟谢文彪
（中南大学信息科学与工程学院，长沙 
4&""%#） 
5/326)：789:6;<=>$:?>$<@

摘要论文介绍了一个基于词频统计的中文分词系统的设计和实现。通过这个系统，可以将输入的连续汉字串进行分

词处理，输出分割后的汉语词串，一般是二字词串，并得到一个词典。词典中不重复地存储了每次处理中得到的词语，以

及这些词语出现的频率。这个系统选用了三种统计原理分别进行统计：互信息， 
.元统计模型和 
A/测试。文中还对这三种

原理的处理结果进行了比较，以分析各种统计原理的统计特点，以及各自所适合的应用场合。

关键词中文分词词频统计互信息 
.元统计模型 
A/测试

文章编号 
&""!/B,,&/（!""#）"%/""C%/"!文献标识码 
D中图分类号 
EF,G, 


!"#$%&% 
’()* 
+%,-%$./.#($ 
0%&%/)1" 
2/&%* 
($ 
+./.#&.#1 
."% 
3)%45%$16 
(7 
."% 
’()* 
3%# 
8($,9#/( 
:/$, 
+($,;#$ 
<"5 
=#/(>5/$ 
=#% 
’%$?#/(


（H@9*132A6*@ 
I<6:@<: 
2@? 
5@+6@::16@+ 
J*)):+: 
*9 
J:@A12) 
I*>A7 
K@6L:1=6AM，J72@+=72 
4&""%#） 
@?&.)/1.：E7: 
N2N:1 
6@A1*?><:= 
A7: 
?:=6+@ 
2@? 
63N):3:@A2A6*@ 
*9 
J76@:=: 
O*1? 
=:+3:@A2A6*@ 
=M=A:3，O76<7 
6= 
P2=:? 
*@ 
=A2A6=A6< 
A7: 
91:Q>:@<M 
*9 
A7: 
O*1?$E71*>+7 
A76= 
=M=A:3，<*@A6@>*>= 
<7212<A:1 
P>@<7 
6@N>A 
<2@ 
P: 
=:+3:@A:?，2@? 
A7:@ 
A7: 
<>A 
2N21A 
O*1? 
P>@<7 
*>AN>A 
<2@ 
P: 
+*AA:@，A7: 
<>A 
2N21A 
O*1? 
P>@<7 
>=>2))M 
6= 
AO* 
<7212<A:1 
O*1? 
P>@<7，2@? 
*@: 
?6<A6*@21M 
<2@ 
P: 
+*AA:@$E7: 
?6<A6*@21M 
=A*1:= 
O*1? 
2@? 
A7: 
91:Q>:@<M 
A72A 
A7: 
O*1? 
2NN:21= 
6@ 
A7:=: 
?6=N*=2) 
A:8A=$E7: 
=:+3:@A2A6*@ 
=M=A:3 
=:):<A= 
A71:: 
R6@?= 
*9 
=A2A6=A6<= 
N16@<6N):= 
A* 
<*>@A 
=:N212A:)M：S>A>2) 
H@9*132A6*@，./0123 
2@? 
A/ 


A:=A$E7: 
N2N:1 
=A6)) 
<*3N21:= 
A7: 
1:=>)A= 
*9 
A7: 
A71:: 
R6@?= 
*9 
N16@<6N):=，2@2)MT:= 
A7: 
?699:1:@<: 
*9 
=A2A6=A6<= 
<7212<A:16=A6<= 
*9 
A7: 
A71:: 
<*>@A6@+ 
N16@<6N):，2@? 
96@?= 
:2<7 
=>6A2P): 
=6A>2A6*@$ 
:%6A()*&：J76@:=: 
O*1? 
=:+3:@A2A6*@，=A2A6=A6< 
A7: 
91:Q>:@<M 
*9 
A7: 
O*1?，3>A>2) 
6@9*132A6*@，./0123，A/A:=A

&引言如公式（&）所示： 


词是最小的能够独立活动的有意义的语言成分，是自然语（#!，"）$)*+! 
%（!，"）（&）
言处理系统中重要的知识载体与基本操作单元。中文分词就是%（!）%（"）

由计算机自动识别文本中词边界的过程，它是中文信息处理最
互信息体现了汉字之间结合关系的紧密程度，当紧密程度
重要的预处理。然而到目前为止，还没有真正成熟实用的中文
高于某一个阈值时，便可认为此字组可能构成了一个词。其中， 


分词系统面世，这成为严重制约中文信息处理发展的瓶颈之一 
’&(。%（!，"）为汉字串 
!"联合出现的概率， 
%（!）为出现汉字串 
!
在此背景下，笔者研究了基于词频统计的中文分词技术的方法
的概率， 
%（"）为汉字串 
"出现的概率，它们在汉字字符串中出
及原理，设计并实现了一个基于词频统计的中文分词系统。 
现的次数分别计为 
&（!）、&（"）、&（!"），&是词频总数，则有公
式（!）： 


%（!，"）$&（!"），%（!）$&（!），%（"）$&（"）（!）

!词频统计中文分词系统的原理&&&
从形式上看，词是稳定的字的组合，因此在上下文中，相邻互信息反映了汉字串 
!"间相关的程度。

的字同时出现的次数越多，就越有可能构成一个词。因此字与（&）如果 
（#!"）!"，即 
%（!"）!%（!）%（"），则 
!"间是正
字相邻共现的频率或概率能够较好地反应成词的可信度。这就相关的，随着 
（#!"）增加，相关度增加，如果 
（#!"）大于给定的
是词频统计的基本原理，这种技术发展至今已经有许多不同的一个阈值，这时可以认为 
!"是一个词；

统计原理。这里就介绍统计的三个原理。 
（!）如果 
（#!"）""，即 
%（!"）"%（!）%（"），则 
!"间是不
!$&互信息原理相关的；
在人们用语言进行交际活动时，语言成分的使用显示出一（,）如果 
（#!"）-"，即 
%（!"）’%（!）%（"），则 
!"间是互斥

定的规律性，因此可使用统计方法对其进行研究统计，语言学的，这时 
!"间基本不会结合成词。 
采用概率论、数理统计以及信息论等数学工具来研究语言成分!$! 
.元统计模型原理 
出现的概率和频率，从而揭示语言的统计规律 
’!(。./0123统计计算语言模型的思想是：一个单词的出现与

定义 
&：对有序汉字串 
!"中汉字 
!"之间的互信息定义其上下文环境中出现的单词序列密切相关，第 
&个词的出现只

基金项目：国家自然科学基金资助（编号： 
C"&%,"4&）；湖南省自然科学基金资助（编号： 
"!UUV!"G4）

作者简介：费洪晓（&GC%/），男，中南大学副教授，主要研究方向为网络管理与网络安全。 


计算机工程与应用 
!""#$% 
C% 




与前面 
!23个词相关，而与其它任何词都不相关，设 
" 
3 
" 
!
. 
"!
是长度为 
!的字串，则字串 
"的似然度用方程表示如公式

（4）所示。 
#（"）$!#（(!) " 
% 
&"%’!53 
" 
%’!5! 
."%23）（4）

%$ 
3 


不难看出，为了预测词 
"!
的出现概率，必须知道它前面
所有词的出现概率。从计算上来看，这种方法太复杂了。如果任
意一个词 
"%
的出现概率只同它前面的两个词有关，问题就可
以得到极大的简化 
647。这时的语言模型叫作三元模型（8/92+/*:），
如公式（;）所示。 


#（"）!#（" 
3）#（" 
!<" 
3）!%=4，$$$，!#（" 
% 
&"%’! 
" 
%’3）（;）

符号 
!%=4，$$$，!#（" 
% 
&"%’! 
" 
%’3）表示概率的连乘。一般来说， 


>元模型就是假设当前词的出现概率只同它前面的 
(23个词
有关。重要的是这些概率参数都是可以通过大规模语料库来计
算的，比如三元概率如公式（#）所示。 


#（" 
% 
&"%’! 
" 
%’3）! 
)*+!,（" 
%’! 
" 
%’3 
"%
）
（#）

)*+!,（" 
%’! 
" 
%’3）

式中 
)*+!,（$$$）表示一个特定词序列在整个语料库中出现

的累计次数。 


!$4 
8测试原理

定义 
!：对有序汉字串 
-./，汉字 
.相对于 
-及 
/的 
8’测试
定义如公式（?）所示。 
,（.）$0（/&.）’0（.&-）（?）

-，/!

"!!（
0（/&.））1!（0（.&-）） 
其中 
0（.&-），0（/&.）分别是 
.关于 
-，/关于 
.的条件概率， 


!!

!（0（.&-）），!（0（/&.））则代表各自的方差，公式（?）中各量可用
公式（%）、（@）估计。 
0（.&-）$0（ 
0（ 
-
-
，.
）
）$ 
（2（2-
-
，.
）
），0（/&.）$0（ 
0（ 
.
.
，
）/
）$（2（2.
.
，
）/
）（%） 


!!（
0（.&-））$ 
（22（ 
-， 
-
.
）
），!!（
0（/&.））$ 
（22（ 
.， 
.
/
）
） 
（@）

!!

从 
82测试的定义，可知：

（3）,-
（，
/.）A"时，字 
.有与后继字 
/相连的趋势，值越大，相
连趋势越强；
（!）（.）="时，不反映任何趋势；


,-，/

（4）,-
（，
/.）B"时，字 
.有与前趋字 
-相连的趋势，值越小，相
连趋势越强。 
词频统计中文分词系统的功能与设计 
4$3词频统计中文分词系统的功能


（3）通过词频统计，对文本（生语料）进行分词，尽可能达到
较高的准确率和较高的运行效率。
（!）能够识别夹杂在文档中的英文单词、数字、标点等特殊
字符。

（4）能将分割出的词语存入一个词典中，并且同一词语不
能重复存储。
（;）能对词典中词语出现的频率进行累加，也就是说，能对
同一个词语在不同的文档中出现的频率进行累加。
（#）能防止对同一文档进行多次处理，以及其它一些错误
操作。 


?@ 
!""#$%计算机工程与应用 



4$!词频统计中文分词系统的设计

这个分词系统主要由三部分组成：预处理模块，词频统计
模块，切分和词典生成模块。首先预处理阶段，利用显式（空格，
分段符等）和隐式的切分标记（标点符号、数字、 
C’D11字符以
及出现频率高、构词能力差的单字词、数词 
5单字常用量词模
式）将待分析的文本切分成短的汉字串。然后通过词频统计模
块，统计出单字出现的频率，以及相邻两字出现的频率，并计算
出相应的统计信息。最后根据词频统计模块中得到的统计信
息，用切分和词典生成模块对文档进行分割，并将切分出的词
语存入词典中，而且将词语出现的频率存入词典中。 


;词频统计中文分词系统的实现 
;$3预处理模块的实现
预处理模块的实现是比较简单的，它的关键问题是汉字的
编码和数据结构。在汉字编码中，每个汉字由两个字节组成，并
且将最高位置 
“3”。这样要判断一个字符是否为汉字，就只要进
行位运算就可以判断。另外在这个系统中要计算某个字符出现
的频率，还要判断它是否能成词，它和其后相邻的字符共现的
频率等。考虑到这些因素，将数据结构设计为以下的形式。
表 
3数据结构

& 
’ 
()*+ 
,-./0 
1 
,-./0!
统计标志字符判断标志单字频率统计信息值相邻 
!字共现频率

其中， 
3就是记录统计信息值，也就是前面所说的互信息
值， 
>元统计预测值， 
82测试值。它是后面词语切分判断的基本
依据。 
;$!词频统计模块的实现

词频统计模块是这个系统一个非常关键的模块，它的实现
算法是由单字出现频率统计和相邻 
!字出现频率统计两个方
面组成。

要计算相邻 
!字共现的频率，这里实现的方法就是每次从
数组 
43!56-中取出两个相邻的汉字，让它们和后面的字符进
行匹配，来计算它们共现的次数，而且对每次统计过的汉字进
行标记。

其相邻 
!字共现频率计算算法流程如图 
3所示。


图 
3相邻 
!字共现频率计算算法流程图 


;$4切分和词典生成模块的实现

这个模块是系统实现的关键，在词频统计模块只计算出单
字出现和相邻 
!字共现的频率，根据这些信息只能直接地判断
出相邻 
!个字是否能组成两字词语。要判断是否能组成 
4字、 
;
字等的长词，则需要进行进一步的判断。图 
!就是它的基本判
断思想。


图 
!基本判断思想

（下转 
3""页） 




位置 
QRS!“确定”。 


O$!操作函数 


O$!$4定义操作函数

操作函数是一个允许以变量为操作对象的操作序列。与其

它语言类似，操作函数在定义时使用形参作为操作对象，在调

用时将实参代替形参。

定义语句格式：

操作函数名（T参数 
4U，.， 
T参数 
"#）$%操作序列 
#。（""4）

一个操作函数可以有 
4个或 
"个参数，这些参数的意义必

须根据操作序列（或序列集）的定义来确定。比如，可以将例 



所定义的操作（形参是一段 
M)*+文档，但未指明具体是哪一

段）序列集命名为操作函数 
&’()*4（+,-&,.），现使用下面例 


44的赋值语句定义该函数。

例 
44：&’()*4（+,-&,.）$#段落 
+,-&,.首部 
!+,-&,.
段首 
$+,-&,.段尾 
!：V菜单 
!%!

“工具”“选项” 
“选项” 
“常
规” 
!“度量单位”：磅!“确定”
；“格式 
”菜单 
!“字体” 
“字体” 
!：“中文字体 
”：仿宋；

%V

“字号”：“字体颜色”蓝色；：“确定”

小五；：“下划线线形”双线 
S!
；“格式”菜单 
!“段落” 
%“段落” 
!：VV“行距”：固定值 
!W0X 
J>Y“设置值”，P磅S；“间距 
R段前” 
Z.磅；“间距 
R段后” 
Z.磅S!
“确定” 
S
例 
4!：在一篇正在被编辑的 
M)*+!""O文档段落中插入一
个 
&行 
.列表格。 
&’()*!（+,-&,.，&，.）$#段落 
+,-&,.中的插入位置： 
7!7“表格”“插入” 
“表格” 
“插入表格” 
V

菜单 
!!%!：“表格尺
寸 
R行数” 
Z&；“表格尺寸 
/列数” 
$.S!“确定”。
在例 
44和例 
4!中，变量 
+,-&,.、&、.只是形式参数，
在函数被应用时，必须代入实参。 


O$!$!调用操作函数

调用操作函数的过程就是将形参换为实参的过程。例如，
假设 
M)*+!""O文档中有一段落 
0，对 
0连续施加 
&’()*4和 
&’()*!操作就会得到这两个函数串行操作的结果。

例 
4O：过程： 


#段落起始位置 
!段落起始位置 
$段落结束位置 
!0 
Z 
1段落起始位置，段落结束位置 
2!&’()*4（0）!&’()*!（0，O，/）。

在这个操作序列中， 
&’()*!（0，O，/）表示在段落 
0中插入
一个 
O行 
/列表， 
&’()*4（0）的意义结合例 
-和例 
44不难得知。 


/结束语

上面所给出的例子虽然未包含 
>M9[7的全部内容，但已
经涉及到语言的各个部分，据此，读者可以看到该语言的使用
方法。以前的计算机语言没用描述用户动作的， 
>M9[7是一
个描述用户动作的语言。使用 
>M9[7语言人们可以用规范、
简练的语言形式描述一个应用，排除对应用过程理解的二义
性，使应用的结果具有确定性。 
>M9[7语言可以应用于计算
机教学和培训方面，事实上，它也是由于教学的需要而产生的。
对于计算机操作水平的网上考试和自动评分，也提供了一种可
用方式。

在未来的研究中，如果能让机器解读 
>M9[7代码段，则
有可能做到用机器模拟人的应用操作序列，让 
>M9[7代码横
跨多个应用软件（包括用户自己开发的应用软件）运行，从而实
现应用的自动化。（收稿日期： 
!""/年 
44月）

参考文献 
4$谭浩强 
$K程序设计 
192$第二版，北京：清华大学出版社， 
4PPP 
!$张丽华等 
$中文 
\]]WK^!""O应用基础教程 
192$冶金工业出版社， 
!""/ 


（上接 
-.页）

在进行切分判断时，是重复地按照这个判断思想来判断。

而且这里设定最长的词语为 
#字词语，所以每次分割要这样重

复地判断 
#次，才能最终判断它是否能组成词语。

系统要求将每个分割出的词语都存入词典中，所以，这里

设计的词典的数据结构如表 
!。

表 
!词典数据结构

& 
’()*+ 
,

存放词语词语出现的频率词语的个数

在存储词语时，还需要先将这个词语与词典中的词语进行
比较，看它是否已经存在词典中。如果已经有了这个词语，就需
要将它的频率进行累加，否则，只要直接增加到词典中去就可
以了。 
/$/三种原理的比较

互信息一般反映的是字与字间的静态结合，因为它计算的
就是相邻字出现的频率，根据这个频率与字单独出现频率进行
比较，计算出互信息来判断什么时候组成词语。 


0元统计模型则有点像天气预报中使用的概率方法，用来
估计概率参数的大规模语料库好比是一个地区历年积累起来
的气象记录。而用三元模型来做天气预报，就好比是根据前两
天的天气情况来预测今天的天气。天气预报当然不可能百分之
百准确，但是笔者大概不会因此就全盘否定这种实用的概率方
法吧。 


4"" 
!""#$%计算机工程与应用 


!测试则能较好地反应字与字之间的动态结合，它是通过
公式计算比较这个字与前面字的结合能力，以及与后一个字的
结合能力，来判断它到底是与那个字结合得更紧密，更能组成
一个词语 
1#2。

但是如果能将互信息和 
3测试这两个统计原理相结合，一
定能起到一个互补的效果，因为它们一个反应的是字与字之间
的静态结合能力，一个反应的是字与字之间的动态结合。所以
它们结合互补的可行性较大。而 
0元统计模型则比较独立，它
在两方面都做得不错，只是计算要比另两个原理都复杂，如果
能在计算上进行改进，将算法的运行效率提高，也是非常有应
用前景的。（收稿日期： 
!""/年 
4"月）

参考文献 
4$5’+) 
6，788 
7$9):3;<=>’:?@8*AB:8+ 
&3C3B:3BDC; 
&8E,8’3C3B)’ 
)F 
GC= 
@C’8:81G2$5@@;BDC3B)’ 
3) 
HC’IB 
507J=055K7，!"""：4/#L4/. 
!$周丽琴，杨季文，吕强 
$基于 
M8N的字词频统计程序的设计与应用 
1G2$
苏州大学学报（自然科学），!""!；4.（4）：O.L// 
O$孙茂松，黄昌宁，邹嘉彦等 
$利用汉字二元语法关系解决汉语自动分
词中的交集型歧义 
1G2$计算机研究与发展， 
4PP%；O/（#）：OO!LOOP 
/$王开铸，李俊杰，吴岩 
$无词典自动分词的研究 
1G2$计算语言学进展与
应用， 
4PP#；4O（!）：O#LO. 
#$丁承，邵志清 
$基于字表的中文搜索引擎分词系统的设计与实现 
1G2$计
算机工程， 
!""4；!%（!）：4P4L4PO



